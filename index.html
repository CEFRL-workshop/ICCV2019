<!DOCTYPE html><!-- 让浏览器得知自己处理的是html -->
<html lang="en">

<!--对文本进行操作	-->
<!--a表示创建超链接，href是链接到的那个东西，target选择新开网页打开链接或者在原网页打开-->
<!--b表示粗体  em表示斜体  s表示删除线-->

<head>
<!--提供有关文档内容和标注信息-->
<meta charset="utf-8"/>
<title>EDLCV</title>
<!--有head必须有title-->
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="Webflow" name="generator"/>
<link href=".\css\CVPR2020.css" rel="stylesheet" type="text/css"/>
<!--链接一个css--> 
<script src=".\css\CVPR2020.js" type="text/javascript"></script> 
<script type="text/javascript">WebFont.load({  google: {    families: ["Roboto:300,regular,500","Roboto Condensed:300,regular,700","Roboto Slab:300,regular,700","Arbutus Slab:regular"]  }});</script> 
<script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
</head>

<body>
<div data-collapse="medium" data-animation="default" data-duration="400" class="navigation w-hidden-main w-nav">
  <div class="w-container"><a href="#" class="brand-link w-nav-brand">
    <div class="logo-text">EDLCV 2020</div>
    </a>
    <nav role="navigation" class="nav-menu w-nav-menu"> <a href="#topics" class="nav-link w-nav-link">Topics</a> <a href="#dates" class="nav-link w-nav-link">Dates</a> <a href="#speakers" class="nav-link w-nav-link">Speakers</a> <a href="#submission" class="nav-link w-nav-link">Submission</a> <a href="#program2" class="nav-link w-nav-link">Program</a> <a href="#awards" class="nav-link w-nav-link">Awards</a> <a href="#organizers" class="nav-link w-nav-link">Organizers</a> </nav>
    <div class="nav-link menu w-nav-button">
      <div class="w-icon-nav-menu"></div>
    </div>
  </div>
</div>
<div data-collapse="medium" data-animation="default" data-duration="400" class="navigation w-nav">
  <div class="w-container"><a href="#" class="brand-link w-nav-brand">
    <div class="logo-text">EDLCV 2020</div>
    </a>
    <nav role="navigation" class="nav-menu w-nav-menu"> <a href="#dates" class="nav-link w-nav-link">Dates</a> <a href="#topics" class="nav-link w-nav-link">Topics</a> <a href="#speakers" class="nav-link w-nav-link">Speakers</a> <a href="#program" class="nav-link w-nav-link">Program</a> <a href="#session1" class="nav-link w-nav-link">Accepted Papers</a> <a href="#submission2" class="nav-link w-nav-link">Submission</a> <a href="#organizers" class="nav-link w-nav-link">Organizers</a> <a href="#awards2" class="nav-link w-nav-link">Previous</a> </nav>
  </div>
</div>
<div data-animation="slide" data-duration="500" data-infinite="1" class="slider w-slider">
  <div class="w-slider-mask">
    <div class="slide _1 w-slide">
      <div class="w-container"></div>
    </div>
    <div class="slide _2 w-slide">
      <div class="w-container"></div>
    </div>
    <div class="slide _3 w-slide">
      <div class="w-container"></div>
    </div>
  </div>
  <div class="w-slider-arrow-left">
    <div class="w-icon-slider-left"></div>
  </div>
  <div class="w-slider-arrow-right">
    <div class="w-icon-slider-right"></div>
  </div>
  <div class="w-slider-nav w-round"></div>
</div>
<div class="section main w-hidden-small w-hidden-tiny">
  <div class="container w-container">
    <h1 class="main-heading">Joint Workshop on <br/>
      Efficient Deep Learning in Computer Vision</h1>
    <div class="text-block-7">June 15th, 2020 <br/>
      Seattle, Washington <br/>
      in conjunction with <a href="http://cvpr2020.thecvf.com/"  class="link">CVPR 2020</a></div>
  </div>
</div>
<div class="section main w-hidden-main w-hidden-medium">
  <div class="container w-container">
    <h1 class="main-heading">Joint Workshop on <br/>
      Efficient Deep Learning in Computer Vision <br/>
    </h1>
    <div class="text-block-7">June 15th, 2020<br/>
      Seattle, Washington <br/>
      in conjunction with <a href="http://cvpr2020.thecvf.com/" class="link">CVPR 2020</a></div>
  </div>
</div>
<div class="section-10 w-hidden-small w-hidden-tiny">
  <div class="w-container">
    <div>
      <div class="text-block-8">Computer Vision has a long history of academic research, and recent advances in deep learning have provided significant improvements in the ability to understand visual content. As a result of these research advances on problems such as object classification, object detection, and image segmentation, there has been a rapid increase in the adoption of Computer Vision in industry; however, mainstream Computer Vision research has given little consideration to speed or computation time, and even less to constraints such as power/energy, memory footprint and model size. The workshop has three main goals on solving and discussing efficiency in Computer Vision: <br/>
        <br/>
        First, the workshop aims to create a venue for a consideration of the new generation of problems that arise as Computer Vision meets mobile and AR/VR systems constraints, to bring together researchers, educators and practitioners who are interested in techniques as well as applications of compact, efficient neural network representations. The workshop discussions will establish close connection between researchers in machine learning and computer vision communities and engineers in industry, and to benefit both academic researchers as well as industrial practitioners. <br/>
        <br/>
        Second, the workshop aims at reproducibility and comparability of methods for compact and efficient neural network representations, and on-device machine learning. Thus a set of benchmarking tasks (image classification, visual question answering) will be provided together with defined data sets, in order to compare the performance of neural network compression methods on the same networks. Submissions are encouraged (but not required) to use these tasks and data sets in their work. Also, contributors are encouraged to make their code available. <br/>
        <br/>
        Third, the workshop aims to discuss the next steps in developing efficient feature representations from three aspects: energy efficient, label efficient, and sample efficient. Despite DNNs are brain-inspired and can achieve or even surpass human-level performance on a variety of challenging computer vision tasks, they continue to trail humans’ abilities in many aspects, such as high energy-efficiency and the ability to perform low-shot learning (learning novel concepts from very few examples). Therefore, the next generation of feature representation and learning techniques should aim to tackle recognition tasks with significantly reduced computational complexity, using as little training data as people need, and to generalize to a range of different tasks beyond the one task the model was trained on. </div>
    </div>
  </div>
</div>
<div class="section-2 w-hidden-main w-hidden-medium">
  <div class="w-container">
    <div>
      <div class="text-block-12 w-hidden-main w-hidden-medium">Computer Vision has a long history of academic research, and recent advances in deep learning have provided significant improvements in the ability to understand visual content. As a result of these research advances on problems such as object classification, object detection, and image segmentation, there has been a rapid increase in the adoption of Computer Vision in industry; however, mainstream Computer Vision research has given little consideration to speed or computation time, and even less to constraints such as power/energy, memory footprint and model size. The workshop has three main goals on solving and discussing efficiency in Computer Vision: <br/>
        First, the workshop aims to create a venue for a consideration of the new generation of problems that arise as Computer Vision meets mobile and AR/VR systems constraints, to bring together researchers, educators and practitioners who are interested in techniques as well as applications of compact, efficient neural network representations. The workshop discussions will establish close connection between researchers in machine learning and computer vision communities and engineers in industry, and to benefit both academic researchers as well as industrial practitioners. <br/>
        Second, the workshop aims at reproducibility and comparability of methods for compact and efficient neural network representations, and on-device machine learning. Thus a set of benchmarking tasks (image classification, visual question answering) will be provided together with defined data sets, in order to compare the performance of neural network compression methods on the same networks. Submissions are encouraged (but not required) to use these tasks and data sets in their work. Also, contributors are encouraged to make their code available. <br/>
        Third, the workshop aims to discuss the next steps in developing efficient feature representations from three aspects: energy efficient, label efficient, and sample efficient. Despite DNNs are brain-inspired and can achieve or even surpass human-level performance on a variety of challenging computer vision tasks, they continue to trail humans’ abilities in many aspects, such as high energy-efficiency and the ability to perform low-shot learning (learning novel concepts from very few examples). Therefore, the next generation of feature representation and learning techniques should aim to tackle recognition tasks with significantly reduced computational complexity, using as little training data as people need, and to generalize to a range of different tasks beyond the one task the model was trained on. <br/>
      </div>
    </div>
  </div>
</div>
<div id="dates" class="section purple w-hidden-small w-hidden-tiny">
  <div class="w-container">
    <h2 class="heading-3">Important Dates</h2>
    <div>
      <div class="w-row">
        <div class="column-2 w-col w-col-5">
          <div class="text-block-9"><strong class="bold-text-3">Paper Submission Deadline:</strong></div>
          <div><strong class="bold-text-4">Notification to authors:</strong></div>
          <div class="text-block-10"><strong class="bold-text-5">Camera ready deadline:</strong></div>
          <div><strong class="bold-text-6">Workshop:</strong></div>
        </div>
        <div class="column-3 w-clearfix w-col w-col-7">
          <div class="text-block-5"><strong class="bold-text-8">March 25, 2020 pst</strong></div>
          <div class="text-block-6 w-clearfix"><strong class="bold-text-9">April 12, 2020 pst</strong></div>
          <div><strong class="bold-text-2">April 19, 2020 pst</strong></div>
          <div><strong class="bold-text-2">June 15, 2020 (Full Day)</strong></div>
        </div>
      </div>
    </div>
  </div>
</div>
<div id="dates" class="section purple w-hidden-main w-hidden-medium">
  <div class="w-container">
    <h2 class="heading-3">Important Dates</h2>
    <div class="row-3 w-row">
      <div class="w-col w-col-6">
        <div><strong class="bold-text-3">Paper Submission Deadline:</strong></div>
        <div><strong class="bold-text-4">Notification to authors:</strong></div>
        <div><strong class="bold-text-5">Camera ready deadline:</strong></div>
        <div><strong class="bold-text-6">Workshop:</strong></div>
      </div>
      <div class="w-col w-col-6">
        <div class="text-block-16"><strong class="bold-text-12"> March 25, 2020 pst</strong></div>
        <div><strong class="bold-text-3">April 12, 2020 pst</strong></div>
        <div><strong class="bold-text-3">April 19, 2020 pst</strong></div>
        <div><strong class="bold-text-3">June 15, 2020 (Full Day)</strong></div>
      </div>
    </div>
  </div>
</div>
<div id="topics" class="section-8">
  <div class="container-4 w-container">
    <h2 class="heading-11">Topics</h2>
    <div class="section-subtitle">
      <ul>
        <li><strong>Efficient Neural Network and Architecture Search</strong></li>
        <ul>
          <li>Compact and efficient neural network architecture for mobile and AR/VR devices</li>
          <li>Hardware (latency, energy) aware neural network architectures search, targeted for mobile and AR/VR devices</li>
          <li>Efficient architecture search algorithm for different vision tasks (detection, segmentation etc.)</li>
          <li>Optimization for Latency, Accuracy and Memory usage, as motivated by embedded devices</li>
        </ul>
        <li><strong>Neural Network Compression</strong></li>
        <ul>
          <li>Model compression (sparsification, binarization, quantization, pruning, thresholding and coding etc.) for efficient inference with deep networks and other ML models</li>
          <li>Scalable compression techniques that can cope with large amounts of data and/or large neural networks (<em>e.g.</em>, not requiring access to complete datasets for hyperparameter tuning and/or retraining)</li>
          <li>Hashing (Binary) Codes Learning</li>
        </ul>
        <li><strong>Low-bit Quantization Network and Hardware Accelerators</strong></li>
        <ul>
          <li>Investigations into the processor architectures (CPU vs GPU vs DSP) that best support mobile applications</li>
          <li>Hardware accelerators to support Computer Vision on mobile and AR/VR platforms</li>
          <li>Low-precision training/inference & acceleration of deep neural networks on mobile devices</li>
        </ul>
        <li><strong>Dataset and benchmark</strong></li>
        <ul>
          <li>Open datasets and test environments for benchmarking inference with efficient DNN representations</li>
          <li>Metrics for evaluating the performance of efficient DNN representations</li>
          <li>Methods for comparing efficient DNN inference across platforms and tasks</li>
        </ul>
        <li><strong>Label/sample/feature efficient learning</strong></li>
        <ul>
          <li>Label Efficient Feature Representation Learning Methods, <em>e.g.</em> Unsupervised Learning, Domain Adaptation, Weakly Supervised Learning and SelfSupervised Learning Approaches</li>
          <li>Sample Efficient Feature Learning Methods, <em>e.g.</em> Meta Learning</li>
          <li>Low Shot learning Techniques</li>
          <li>New Applications, <em>e.g.</em> Medical Domain</li>
        </ul>
        <li><strong>Mobile and AR/VR Applications</strong></li>
        <ul>
          <li>Novel mobile and AR/VR applications using Computer Vision such as image processing (<em>e.g.</em> style transfer, body tracking, face tracking) and augmented reality</li>
          <li>Learning efficient deep neural networks under memory and computation constraints for on-device applications</li>
        </ul>
      </ul>
    </div>
  </div>
</div>
<div id="speakers" class="section-3 w-hidden-small w-hidden-tiny">
  <div class="w-container">
    <h2 class="heading-5">Keynote Speakers</h2>
    <div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/diana.png" width="64" alt="" class="person"/>
          <h1 class="heading">Prof. Diana Marculescu<br/>
            CMU</h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3">Title: <strong>TBD <!--(--></strong><!--<a target="_blank" href="" class="link-13"><strong>Slides</strong></a><strong>)</strong><br/>--> 
            <br/>
            <br/>
            Biography: Diana Marculescu received a degree in computer science from Politehnica University of Bucharest, Romania, in 1991, and a Ph.D. in computer engineering from the University of Southern California in 1998. From 2014 to 2018, she served as Associate Department Head for Academic Affairs in Electrical and Computer Engineering, and, from 2015 to 2019, was the founding director of the College of Engineering Center for Faculty Success. In 2019, the Cockrell School of Engineering at the University of Texas at Austin, had named her New Chair of Electrical and Computer Engineering.</div>
        </div>
      </div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/vivienne.jpg" width="64" alt="" class="person"/>
          <h1 class="heading">Prof. Vivienne Sze<br/>
            MIT</h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3">Title: <strong>TBD <!--(--></strong><!--<a target="_blank" href="" class="link-13"><strong>Slides</strong></a><strong>)</strong><br/>--> 
            <br/>
            <br/>
            Biography: Vivienne Sze received the B.A.Sc. (Hons) degree in electrical engineering from the University of Toronto, Toronto, ON, Canada, in 2004, and the S.M. and Ph.D. degree in electrical engineering from the Massachusetts Institute of Technology (MIT), Cambridge, MA, in 2006 and 2010 respectively. She received the Jin-Au Kong Outstanding Doctoral Thesis Prize in electrical engineering at MIT in 2011. She is currently an Associate Professor in the Electrical Engineering and Computer Science Department at MIT. Her research interests include energy-efficient algorithms and architectures for portable multimedia applications. From September 2010 to July 2013, she was a Member of Technical Staff in the Systems and Applications R&D Center at Texas Instruments (TI), Dallas, TX, where she designed low-power algorithms and architectures for video coding. She also represented TI in the JCT-VC committee of ITU-T and ISO/IEC standards body during the development of High Efficiency Video Coding (HEVC), which received a Primetime Emmy Engineering Award. Within the committee, she was the primary coordinator of the core experiment on coefficient scanning and coding. She is a recipient of the 2017 Qualcomm Faculty Award, the 2016 Google Faculty Research Award, the 2016 AFOSR Young Investigator Research Program (YIP) Award, the 2016 3M Non-Tenured Faculty Award, the 2014 DARPA Young Faculty Award, the 2007 DAC/ISSCC Student Design Contest Award, and a co-recipient of the 2017 CICC Outstanding Invited Paper Award, the 2016 IEEE Micro Top Picks Award and the 2008 A-SSCC Outstanding Design Award.</div>
        </div>
      </div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/Songhan.png" width="64" alt="" class="person"/>
          <h1 class="heading">Song Han<br/>
            MIT, the United States</h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3">Title: <strong>TBD<!--(--></strong><!--<a target="_blank" href="" class="link-11"><strong>Slides</strong></a><strong>)<br/>--> 
            <br/>
            <br/>
            </strong>Biography: Song Han is an assistant professor at MIT EECS. Dr. Han received the Ph.D. degree in Electrical Engineering from Stanford advised by Prof. Bill Dally. Dr. Han’s research focuses on efficient deep learning computing. He proposed “Deep Compression” and “Efficient Inference Engine” that impacted the industry. His work received the best paper award in ICLR’16 and FPGA’17. He is the co-founder and chief scientist of DeePhi Tech (a leading efficient deep learning solution provider), which was acquired by Xilinx. The pruning, compression and acceleration techniques have been integrated into products.</div>
        </div>
      </div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/chelsea.png" width="64" alt="" class="person"/>
          <h1 class="heading">Chelsea Finn<br/>
            Stanford University, <br/>
            the United States </h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3">Title: <strong>TBD<!--(--></strong><!--<a target="_blank" href="" class="link-13"><strong>Slides</strong></a><strong>)</strong><br/>--> 
            <br/>
            <br/>
            Biography: Chelsea Finn completed her Ph.D. in computer science at UC Berkeley and her B.S. in electrical engineering and computer science at MIT. Now she is a research scientist at Google Brain, a post-doc at Berkeley AI Research Lab (BAIR), and an acting assistant professor at Stanford. She will join the Stanford Computer Science faculty full time, starting in Fall 2019. She is interested in how algorithms can enable machines to acquire more general notions of intelligence through learning and interaction, allowing them to autonomously learn a variety of complex sensorimotor skills in real-world settings. This includes learning deep representations for representing complex skills from raw sensory inputs, enabling machines to learn through interaction without human supervision, and allowing systems to build upon what they’ve learned previously to acquire new capabilities with small amounts of experience.</div>
        </div>
      </div>
    </div>
  </div>
</div>
<div id="speakers" class="section-3 w-hidden-main w-hidden-medium">
  <div class="w-container">
    <h2 class="heading-10">Keynote Speakers</h2>
    <div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/diana.png" width="64" alt="" class="person"/>
          <h1 class="heading">Prof. Diana Marculescu<br/>
            CMU</h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3">Title: <strong>TBD <!--(--></strong><!--<a target="_blank" href="" class="link-13"><strong>Slides</strong></a><strong>)</strong><br/>--> 
            <br/>
            <br/>
            Biography: Diana Marculescu received a degree in computer science from Politehnica University of Bucharest, Romania, in 1991, and a Ph.D. in computer engineering from the University of Southern California in 1998. From 2014 to 2018, she served as Associate Department Head for Academic Affairs in Electrical and Computer Engineering, and, from 2015 to 2019, was the founding director of the College of Engineering Center for Faculty Success. In 2019, the Cockrell School of Engineering at the University of Texas at Austin, had named her New Chair of Electrical and Computer Engineering.</div>
        </div>
      </div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/vivienne.jpg" width="64" alt="" class="person"/>
          <h1 class="heading">Prof. Vivienne Sze<br/>
            MIT</h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3">Title: <strong>TBD <!--(--></strong><!--<a target="_blank" href="" class="link-13"><strong>Slides</strong></a><strong>)</strong><br/>--> 
            <br/>
            <br/>
            Biography: Vivienne Sze received the B.A.Sc. (Hons) degree in electrical engineering from the University of Toronto, Toronto, ON, Canada, in 2004, and the S.M. and Ph.D. degree in electrical engineering from the Massachusetts Institute of Technology (MIT), Cambridge, MA, in 2006 and 2010 respectively. She received the Jin-Au Kong Outstanding Doctoral Thesis Prize in electrical engineering at MIT in 2011. She is currently an Associate Professor in the Electrical Engineering and Computer Science Department at MIT. Her research interests include energy-efficient algorithms and architectures for portable multimedia applications. From September 2010 to July 2013, she was a Member of Technical Staff in the Systems and Applications R&D Center at Texas Instruments (TI), Dallas, TX, where she designed low-power algorithms and architectures for video coding. She also represented TI in the JCT-VC committee of ITU-T and ISO/IEC standards body during the development of High Efficiency Video Coding (HEVC), which received a Primetime Emmy Engineering Award. Within the committee, she was the primary coordinator of the core experiment on coefficient scanning and coding. She is a recipient of the 2017 Qualcomm Faculty Award, the 2016 Google Faculty Research Award, the 2016 AFOSR Young Investigator Research Program (YIP) Award, the 2016 3M Non-Tenured Faculty Award, the 2014 DARPA Young Faculty Award, the 2007 DAC/ISSCC Student Design Contest Award, and a co-recipient of the 2017 CICC Outstanding Invited Paper Award, the 2016 IEEE Micro Top Picks Award and the 2008 A-SSCC Outstanding Design Award.</div>
        </div>
      </div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/Songhan.png" width="64" alt="" class="person"/>
          <h1 class="heading">Song Han<br/>
            MIT, the United States</h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3">Title: <strong>TBD<!--(--></strong><!--<a target="_blank" href="" class="link-11"><strong>Slides</strong></a><strong>)<br/>--> 
            <br/>
            <br/>
            </strong>Biography: Song Han is an assistant professor at MIT EECS. Dr. Han received the Ph.D. degree in Electrical Engineering from Stanford advised by Prof. Bill Dally. Dr. Han’s research focuses on efficient deep learning computing. He proposed “Deep Compression” and “Efficient Inference Engine” that impacted the industry. His work received the best paper award in ICLR’16 and FPGA’17. He is the co-founder and chief scientist of DeePhi Tech (a leading efficient deep learning solution provider), which was acquired by Xilinx. The pruning, compression and acceleration techniques have been integrated into products.</div>
        </div>
      </div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/chelsea.png" width="64" alt="" class="person"/>
          <h1 class="heading">Chelsea Finn<br/>
            Stanford University, <br/>
            the United States </h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3"> Title: <strong>TBD <!--(--></strong><!--<a target="_blank" href="" class="link-13"><strong>Slides</strong></a><strong>)</strong><br/>--> 
            <br/>
            <br/>
            Biography: Chelsea Finn completed her Ph.D. in computer science at UC Berkeley and her B.S. in electrical engineering and computer science at MIT. Now she is a research scientist at Google Brain, a post-doc at Berkeley AI Research Lab (BAIR), and an acting assistant professor at Stanford. She will join the Stanford Computer Science faculty full time, starting in Fall 2019. She is interested in how algorithms can enable machines to acquire more general notions of intelligence through learning and interaction, allowing them to autonomously learn a variety of complex sensorimotor skills in real-world settings. This includes learning deep representations for representing complex skills from raw sensory inputs, enabling machines to learn through interaction without human supervision, and allowing systems to build upon what they’ve learned previously to acquire new capabilities with small amounts of experience.</div>
        </div>
      </div>
    </div>
  </div>
</div>
<div id="program" class="section w-hidden-small w-hidden-tiny">
  <div class="w-container">
    <h2 id="program" class="heading-6">Program (Tentative)</h2>
    <div class="text-block-17"> (Location: <a href="" target="_blank" class="link-8"> TBD</a> <a href="" target="_blank" class="link-9"> </a>) </div>
    <div>
      <div class="html-embed w-embed">
        <table border="1" width="100%" align="center">
          <tr>
            <th font size="1" bgcolor="#808080" align="center" colspan="2" >Presentation Time</th>
            <th font size="1" bgcolor="#808080" align="center" >Event</th>
            <th font size="1" bgcolor="#808080" align="center" >Title</th>
            <th font size="1" bgcolor="#808080" align="center" >Paper ID</th>
            <th font size="1" bgcolor="#808080" align="center" >Duration</th>
          </tr>
          <tr align="center" >
            <td>08:30 ~ 08:40</td>
            <td>/</td>
            <td>Welcome Introduction</td>
            <td>/</td>
            <td></td>
            <td>10 min</td>
          </tr>
          <tr align="center" >

            <td>08:40 ~ 09:20</td>
            <td>20:40 ~ 21:20</td>
            <td>Keynote Talk 1</td>
            <td>Prof. Diana Marculescu, CMU</td>
            <td></td>
            <td>40 min</td>
          </tr>
          <tr align="center" >

            <td>09:20 ~ 10:00</td>
            <td>21:20 ~ 22:00</td>
            <td>Keynote Talk 2</td>
            <td>Prof. Vivienne Sze, MIT</td>
            <td></td>
            <td>40 min</td>
          </tr>
          <tr align="center" >

            <td>10:00 ~ 10:10</td>
            <td>22:00 ~ 22:10</td>
            <td>Coffee Break</td>
            <td>/</td>
            <td></td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>10:10 ~ 10:30</td>
            <td>22:10 ~ 22:30</td>
            <td rowspan="4">Oral Session 1</td>
            <td>Randaugment: Practical automated data augmentation with a reduced search space</td>
            <td>27</td>
            <td>20 min</td>
          </tr>
          <tr align="center" >
            <td>10:30 ~ 10:50</td>
            <td>22:30 ~ 22:50</td>
            <td>Neural Network Compression Using Higher-Order Statistics and Auxiliary Reconstruction Losses</td>
            <td>39</td>
            <td>20 min</td>
          </tr>
          <tr align="center" >
            <td>10:50 ~ 11:10</td>
            <td>22:50 ~ 23:10</td>
            <td>Dithered backprop: A sparse and quantized backpropagation algorithm for more efficient deep neural network training</td>
            <td>43</td>
            <td>20 min</td>
          </tr>
          <tr align="center" >
            <td>11:10 ~ 11:30</td>
            <td>23:10 ~ 23:30</td>
            <td>Learning Sparse & Ternary Neural Networks with Entropy-Constrained Trained Ternarization (EC2T)</td>
            <td>44</td>
            <td>20 min</td>
          </tr>
          <tr align="center" >
            <td>11:30 ~ 12:30</td>
            <td>23:30 ~ 00:30</td>
            <td>Lunch</td>
            <td>/</td>
            <td></td>
            <td>60 min</td>
          </tr>
          <tr align="center" >
            <td>12:30 ~ 12:40</td>
            <td>00:30 ~ 00:40</td>
            <td  rowspan="9">Spotlight 1</td>
            <td>Learning Low-rank Deep Neural Networks via Singular Vector Orthogonality Regularization and Singular Value Sparsification</td>
            <td>4</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>12:40 ~ 12:50</td>
            <td>00:40 ~ 00:50</td>
            <td>FoNet: A Memory-efficient Fourier-based Orthogonal Network for Object Recognition</td>
            <td>15</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>12:50 ~ 13:00</td>
            <td>00:50 ~ 01:00</td>
            <td>LSQ+: Improving low-bit quantization through learnable offsets and better initialization</td>
            <td>20</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>13:00 ~ 13:10</td>
            <td>01:00 ~ 01:10</td>
            <td>Least squares binary quantization of neural networks</td>
            <td>22</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>13:10 ~ 13:20</td>
            <td>01:10 ~ 01:20</td>
            <td>Any-Width Networks</td>
            <td>29</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>13:20 ~ 13:30</td>
            <td>01:20 ~ 01:30</td>
            <td>Data-Free Network Quantization With Adversarial Knowledge Distillation</td>
            <td>35</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>13:30 ~ 13:40</td>
            <td>01:30 ~ 01:40</td>
            <td>Structured Weight Unification and Encoding for Neural Network Compression and Acceleration</td>
            <td>38</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>13:40 ~ 13:50</td>
            <td>01:40 ~ 01:50</td>
            <td>Intelligent Scene Caching to Improve Accuracy for Energy-Constrained Embedded Vision</td>
            <td>46</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>13:50 ~ 14:00</td>
            <td>01:50 ~ 02:00</td>
            <td>Adaptive Posit: Parameter aware numerical format for deep learning inference on the edge</td>
            <td>52</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>14:00 ~ 14:30</td>
            <td>02:00 ~ 02:30</td>
            <td>Q & A</td>
            <td>/</td>
            <td></td>
            <td>30 min</td>
          </tr>
          <tr align="center" >
            <td>14:30 ~ 15:10</td>
            <td>02:30 ~ 03:10</td>
            <td>Keynote Talk 3</td>
            <td>Song Han, MIT</td>
            <td></td>
            <td>40 min</td>
          </tr>
          <tr align="center" >
            <td>15:10 ~ 15:50</td>
            <td>03:10 ~ 03:50</td>
            <td>Keynote Talk 4</td>
            <td>Chelsea Finn, Stanford University</td>
            <td></td>
            <td>40 min</td>
          </tr>
          <tr align="center" >
            <td>15:50 ~ 16:00</td>
            <td>03:50 ~ 04:00</td>
            <td>Coffee Break</td>
            <td>/</td>
            <td></td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>16:00 ~ 16:20</td>
            <td>04:00 ~ 04:20</td>
            <td rowspan="5">Oral Session 2</td>
            <td>BAMSProd: A Step towards Generalizing the Adaptive Optimization Methods to Deep Binary Model</td>
            <td>1</td>
            <td>20 min</td>
          </tr>
          <tr align="center" >
            <td>16:20 ~ 16:40</td>
            <td>04:20 ~ 04:40</td>
            <td>Dynamic Inference: A New Approach Toward Efficient Video Action Recognition</td>
            <td>3</td>
            <td>20 min</td>
          </tr>
          <tr align="center" >
            <td>16:40 ~ 17:00</td>
            <td>04:40 ~ 05:00</td>
            <td>Low-bit Quantization Needs Good Distribution</td>
            <td>7</td>
            <td>20 min</td>
          </tr>
          <tr align="center" >
            <td>17:00 ~ 17:20</td>
            <td>05:00 ~ 05:20</td>
            <td>Attentive Semantic Preservation Network for Zero-Shot Learning</td>
            <td>9</td>
            <td>20 min</td>
          </tr>
          <tr align="center" >
            <td>17:20 ~ 17:40</td>
            <td>05:20 ~ 05:40</td>
            <td>AdaMT-Net: An Adaptive Weight Learning Based Multi-Task Learning Model For Scene Understanding</td>
            <td>31</td>
            <td>20 min</td>
          </tr>
          <tr align="center" >
            <td>17:40 ~ 18:40</td>
            <td>05:40 ~ 06:40</td>
            <td>Dinner</td>
            <td>/</td>
            <td></td>
            <td>60min</td>
          </tr>
          <tr align="center" >
            <td>18:40 ~ 18:50</td>
            <td>06:40 ~ 06:50</td>
            <td rowspan="9">Spotlight 2</td>
            <td>Mimic The Raw Domain: Accelerating Action Recognition in the CompressedDomain</td>
            <td>11</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>18:50 ~ 19:00</td>
            <td>06:50 ~ 07:00</td>
            <td>Constraint-Aware Importance Estimation for Global Filter Pruning under Multiple Resource Constraints</td>
            <td>13</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>19:00 ~ 19:10</td>
            <td>07:00 ~ 07:10</td>
            <td>Computer-aided diagnosis system of lung carcinoma using Convolutional Neural Networks</td>
            <td>16</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>19:10 ~ 19:20</td>
            <td>07:10 ~ 07:20</td>
            <td>Fast Hardware-Aware Neural Architecture Search</td>
            <td>18</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>19:20 ~ 19:30</td>
            <td>07:20 ~ 07:30</td>
            <td>Learning Sparse Neural Networks Through Mixture-Distributed Regularization</td>
            <td>19</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>19:30 ~ 19:40</td>
            <td>07:30 ~ 07:40</td>
            <td>RefineDetLite: A Lightweight One-stage Object Detection Framework for CPU-only Devices</td>
            <td>23</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>19:40 ~ 19:50</td>
            <td>07:40 ~ 07:50</td>
            <td>Ternary MobileNets via Per-Layer Hybrid Filter Banks</td>
            <td>34</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>19:50 ~ 20:00</td>
            <td>07:50 ~ 08:00</td>
            <td>Now that I can see, I can improve : Enabling data-driven finetuning of CNNs on the edge</td>
            <td>36</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>20:00 ~ 20:10</td>
            <td>08:00 ~ 08:10</td>
            <td>Monte Carlo Gradient Quantization</td>
            <td>42</td>
            <td>10 min</td>
          </tr>
          <tr align="center" >
            <td>20:10 ~ 20:40</td>
            <td>08:10 ~ 08:40</td>
            <td>Q & A</td>
            <td>/</td>
            <td></td>
            <td>30 min</td>
          </tr>
        </table>
      </div>
    </div>
  </div>
</div>

<!--table表示创建表格  tr表示一行中的单元格 td表示单元格  th表示标题--> 
<!--thead表示表头  tbody表示表的内容  tfoot表示表脚--> 
<!--br换行 没有/br--> 
<!--colspan表示合并行单元格  rowspan表示合并列单元格-->

<div id="program2" class="section w-hidden-main w-hidden-medium">
  <div class="w-container">
    <h2 id="program" class="heading-8">Program (Tentative)</h2>
    <div class="text-block-17"> (Location: <a href="" target="_blank" class="link-8"> TBD </a> <a href="" target="_blank" class="link-9"> </a>) </div>
    <div>
      <div class="html-embed-2 w-embed">
        <table  border="1" width="300" align="center" >
          <tr>
            <th font size="1" bgcolor="#808080" align="center" > <font color="white" > <font size="1"> Time</font> </th>
            <th bgcolor="#808080" align="center" > <font color="white"><font size="1">Event</font> </th>
          </tr>
          <tr align="center" >
            <td><font size="1">8:50 - 9:00 </font></td>
            <td><font size="1">Welcome by organizers</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">9:00 - 9:30 </font></td>
            <td><b><font size="1">Invited talk: Prof. Philip Torr (Oxford University)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">9:30 - 10:00 </font></td>
            <td><b><font size="1">Invited talk: Prof. Nic Lane (Oxford University)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">10:00 - 10:30 </font></td>
            <td><font size="1">Coffee break</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">10:30 - 11:00</font></td>
            <td><b><font size="1">Oral Session 1 (3  presentations: 10min each)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">11:00 - 11:30</font></td>
            <td><b><font size="1">Keynote talk: Prof. Song Han (MIT)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">11:30 - 12:00 </font></td>
            <td><b><font size="1">Invited talk: Prof. Diana Marculescu (CMU)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">12:00 - 12:30 </font></td>
            <td><b><font size="1">Oral Session 2 (3  presentations: 10min each)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">12:30 - 13:30 </font></td>
            <td><font size="1"> Lunch break</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">13:30 - 14:00 </font></td>
            <td><b><font size="1">Keynote talk: Prof. Bill Dally (Stanford)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">14:00 - 14:30 </font></td>
            <td><b><font size="1">Invited talk: Prof. Chelsea Finn (Stanford)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">14:30 - 15:00 </font></td>
            <td><b><font size="1">Oral Session 3 (3  presentations: 10min each)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">15:00 - 16:00 </font></td>
            <td><b><font size="1">Poster session by paper submission</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">16:00 - 17:30 </font></td>
            <td><b><font size="1">Panel presentations and discussion on Efficient deep learning algorithms. Moderator: Luc Van Gool</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">17:30 - 17:45 </font></td>
            <td><b><font size="1">Closing awards for best paper and best poster</font></td>
          </tr>
        </table>
      </div>
    </div>
  </div>
</div>
<div id="session1" class="section-9">
  <div class="w-container">
    <h1 class="heading-5" align="center">Accepted Papers(TBD)</h1>
  </div>
</div>
<div class="section-6 w-hidden-small w-hidden-tiny">
  <div class="w-container">
    <div>
      <h2 class="heading-7" align="center"> Awards </h2>
      <!--<div class="text-block-14">EDLCV 2020 will announce one <strong><em class="italic-text-5">Best Paper Award</em></strong> and one <strong><em class="italic-text-7">Best</em></strong><span> <strong><em class="italic-text-6">Paper Honorable Mention Award</em></strong></span>, fully sponsored by the <a href="https://www.inceptioniai.org/"><strong><em class="italic-text-2">Inception Institute of Artificial Intelligence</em></strong></a>:<br/>
        <br/>
        ※ EDLCV 2020 Best Paper Award ($1,500)<br/>
        <strong></strong><em><br/>
        ‍<br/>
        ※ </em>EDLCV 2020 Best Paper Honorable Mention Award ($500)<br/>
        <strong></strong><em><br/>
        ‍</em></div>--> 
    </div>
  </div>
</div>
<div class="section-6 w-hidden-main w-hidden-medium">
  <div class="w-container">
    <div class="div-block">
      <h2 class="heading-7" align="center">Awards</h2>
      <!--<div class="text-block-14">EDLCV 2020 will announce one <strong><em class="italic-text-5">Best Paper Award</em></strong> and one <strong><em class="italic-text-7">Best</em></strong><span> <strong><em class="italic-text-6">Paper Honorable Mention Award</em></strong></span>, fully sponsored by the <a href="https://www.inceptioniai.org/"><strong><em class="italic-text-2">Inception Institute of Artificial Intelligence</em></strong></a>:<br/>
        <br/>
        ※ EDLCV 2020 Best Paper Award ($1,500)<br/>
        <strong></strong><em><br/>
        ‍<br/>
        ※ </em>EDLCV 2020 Best Paper Honorable Mention Award ($500)<br/>
        <strong></strong><em><br/>
        ‍</em></div>--> 
    </div>
  </div>
</div>
<div id="submission" class="section-7 w-hidden-main w-hidden-medium w-hidden-small w-hidden-tiny">
  <div id="awards" class="w-container">
    <div>
      <h2 class="heading-12">Submission</h2>
      <div class="text-block-15">
        <p>All submissions will be handled electronically via the workshop’s CMT Website.  Click the following link to go to the submission site: <a href="https://cmt3.research.microsoft.com/EDLCV2020/" class="newfont">https://cmt3.research.microsoft.com/EDLCV2020/</a></p>
        <p>Papers should describe original and unpublished work about the related topics. Each paper will receive double blind reviews, moderated by the workshop chairs. Authors should take into account the following:</p>
        <ul>
          <li>All papers must be written and presented in English.</li>
          <li>All papers must be submitted in PDF format. The workshop paper format guidelines are the same as the <a href="http://cvpr2020.thecvf.com/submission/main-conference/author-guidelines#submission-guidelines" class="newfont">Main Conference papers</a></li>
          <li>The maximum paper length is 8 pages (excluding references). Note that shorter submissions are also welcome.</li>
          <li>The accepted papers will be published in CVF open access as well as in IEEE Xplore.</li>
        </ul>
      </div>
    </div>
  </div>
</div>
<div id="submission2" class="section-7">
  <div id="awards" class="w-container">
    <div>
      <h2 class="heading-12">Submission</h2>
      <div class="text-block-15">
        <p>All submissions will be handled electronically via the workshop’s CMT Website.  Click the following link to go to the submission site: <a href="https://cmt3.research.microsoft.com/EDLCV2020/" class="newfont">https://cmt3.research.microsoft.com/EDLCV2020/</a></p>
        <p>Papers should describe original and unpublished work about the related topics. Each paper will receive double blind reviews, moderated by the workshop chairs. Authors should take into account the following:</p>
        <ul>
          <li>All papers must be written and presented in English.</li>
          <li>All papers must be submitted in PDF format. The workshop paper format guidelines are the same as the <a href="http://cvpr2020.thecvf.com/submission/main-conference/author-guidelines#submission-guidelines" class="newfont">Main Conference papers</a></li>
          <li>The maximum paper length is 8 pages (excluding references). Note that shorter submissions are also welcome.</li>
          <li>The accepted papers will be published in CVF open access as well as in IEEE Xplore.</li>
        </ul>
      </div>
    </div>
  </div>
</div>
<div id="organizers" class="section">
  <div class="w-container">
    <h2 class="heading-13">Organizers</h2>
    <div class="row-4 w-row">
      <table>
        <tr>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="http://www.ee.oulu.fi/~lili/LiLiuHomepage.html" class="w-inline-block"><img src="pictures/li.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Li Liu<br/>
            University of Oulu &amp; NUDT</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://homes.esat.kuleuven.be/~yliu/" class="w-inline-block"><img src="pictures/yuliu.png" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Yu Liu<br/>
            PSI group of KU Leuven</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://wlouyang.github.io/" class="w-inline-block"><img src="pictures/wanli.png" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Wanli Ouyang<br/>
            Univeristy of Sydney</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/" class="w-inline-block"><img src="pictures/jiwen.png" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Jiwen Lu<br/>
            Tsinghua University</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://scholar.google.com/citations?user=bjEpXBoAAAAJ&amp;hl=en" class="w-inline-block"><img src="pictures/matti.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Prof. Matti Pietikäinen<br/>
            University of Oulu</h1>
          </a>
          </td>
        </tr>
        <tr>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://www.vision.ee.ethz.ch/en/members/get_member.cgi?id=1" class="w-inline-block"><img src="pictures/luc.png" width="64" alt="" class="person"/>
          <h1 class="heading-2">Prof. Luc Van Gool<br/>
            ETH Zurich</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://research.fb.com/people/vajda-peter/" class="w-inline-block"><img src="pictures/peter.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Peter Vajda<br/>
            Research Scientist Manager <br/>
            at Facebook</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://research.fb.com/people/zhang-peizhao/" class="w-inline-block"><img src="pictures/zhang.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Peizhao Zhang<br/>
            Research Scientist <br/>
            at Facebook</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://research.fb.com/people/zhang-peizhao/" class="w-inline-block"><img src="pictures/pete.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Pete Warden<br/>
            Staff Research Engineer <br/>
            at Google</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://people.eecs.berkeley.edu/~keutzer/" class="w-inline-block"><img src="pictures/kurt.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Prof. Kurt Keutzer<br/>
            UC Berkeley</h1>
          </a>
          </td>
        </tr>
        <tr>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://www.jonathandekhtiar.eu/" class="w-inline-block"><img src="pictures/jonathan.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Jonathan Dekhtiar<br/>
            Deep Learning Engineer <br/>
            at Nvidia</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="http://iphome.hhi.de/samek/" class="w-inline-block"><img src="pictures/wojciech.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Wojciech Samek<br/>
            Fraunhofer Heinrich Hertz Institute</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://eiclab.net/" class="w-inline-block"><img src="pictures/lin.png" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Yingyan Lin<br/>
            Rice University</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://www.joanneum.at/digital/das-institut/team/detail/employee/bailer/" class="w-inline-block"><img src="pictures/werner.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Werner Bailer<br/>
            Joanneum Research</h1>
          </a>
          </td>
        </tr>
      </table>
    </div>
  </div>
</div>
<div id="awards" class="section footer w-hidden-main w-hidden-medium">
  <div id="awards" class="container-2 w-container">
    <h2 class="heading-5">Previous EDLCV Workshop</h2>
    <div class="text-block-2 link">
      <ul>
        <li> <a href="http://www.ee.oulu.fi/~lili/CEFRLatICCV2019.html" class="newfont">4<sup>th</sup> CEFRL Workshop in conjunction with ICCV 2019</a></li>
        <li> <a href="http://www.ee.oulu.fi/~lili/CEFRLatCVPR2019.html" class="newfont">3<sup>rd</sup> CEFRL Workshop in conjunction with CVPR 2019</a></li>
        <li> <a href="https://cefrl.webflow.io/" class="newfont">2<sup>nd</sup> CEFRL Workshop in conjunction with ECCV 2018</a></li>
        <li> <a href="http://www.ee.oulu.fi/~lili/ICCVW2017.html" class="newfont">1<sup>st</sup> CEFRL Workshop in conjunction with ICCV 2017</a></li>
        <li><a href="https://icml.cc/Conferences/2019/Schedule?showEvent=3520" class="newfont">ICML 2019: Joint Workshop on On-Device Machine Learning and Compact Deep Neural Network Representations (ODML-CDNNR)</a></li>
        <li> <a href="https://nips.cc/Conferences/2018/Schedule?showEvent=10941" class="newfont">NIPS 2018: workshop on Compact Deep Neural Networks with industrial applications</a></li>
        <li><a href="https://sites.google.com/view/ecv2019/" class="newfont">2<sup>nd</sup> Efficient Deep Learning for Computer Vision
          in conjunction with CVPR 2019</a></li>
        <li><a href="https://sites.google.com/view/ecv2018/" class="newfont">1<sup>st</sup> Efficient Deep Learning for Computer Vision
          in conjunction with CVPR 2018</a></li>
      </ul>
    </div>
  </div>
</div>
<div id="awards2" class="section footer w-hidden-small w-hidden-tiny">
  <div id="awards" class="w-container">
    <div class="w-container">
      <h2 class="heading-5">Previous EDLCV Workshop</h2>
      <div class="text-block-2 link">
        <ul>
          <li> <a href="http://www.ee.oulu.fi/~lili/CEFRLatICCV2019.html" class="newfont">4<sup>th</sup> CEFRL Workshop in conjunction with ICCV 2019</a></li>
          <li> <a href="http://www.ee.oulu.fi/~lili/CEFRLatCVPR2019.html" class="newfont">3<sup>rd</sup> CEFRL Workshop in conjunction with CVPR 2019</a></li>
          <li> <a href="https://cefrl.webflow.io/" class="newfont">2<sup>nd</sup> CEFRL Workshop in conjunction with ECCV 2018</a></li>
          <li> <a href="http://www.ee.oulu.fi/~lili/ICCVW2017.html" class="newfont">1<sup>st</sup> CEFRL Workshop in conjunction with ICCV 2017</a></li>
          <li><a href="https://icml.cc/Conferences/2019/Schedule?showEvent=3520" class="newfont">ICML 2019: Joint Workshop on On-Device Machine Learning and Compact Deep Neural Network Representations (ODML-CDNNR)</a></li>
          <li> <a href="https://nips.cc/Conferences/2018/Schedule?showEvent=10941" class="newfont">NIPS 2018: workshop on Compact Deep Neural Networks with industrial applications</a></li>
          <li><a href="https://sites.google.com/view/ecv2019/" class="newfont">2<sup>nd</sup> Efficient Deep Learning for Computer Vision
            in conjunction with CVPR 2019</a></li>
          <li><a href="https://sites.google.com/view/ecv2018/" class="newfont">1<sup>st</sup> Efficient Deep Learning for Computer Vision
            in conjunction with CVPR 2018</a></li>
        </ul>
      </div>
    </div>
  </div>
</div>
<div class="section-6 w-hidden-small w-hidden-tiny">
<div class="w-container">
  <div>
    <h2 class="heading-7" align="center">Main Contacts</h2>
    <div class="newfontsize">If you have question, please contact : </br>
      </br>
      <ul>
        <li><strong><em>Dr. Li Liu</em></strong> : li.liu@oulu.fi</li>
        </br>
        <li><strong><em >Dr. Peter Vajda</em></strong> : vajdap@fb.com </li>
        </br>
        <li><strong><em>Dr. Werner Bailer</em></strong> : werner.bailer@joanneum.at</li>
      </ul>
      </br>
    </div>
  </div>
</div>
<div class="section-6 w-hidden-main w-hidden-medium">
  <div class="w-container">
    <div class="div-block">
      <h2 class="heading-7" align="center">Main Contacts</h2>
      <div class="newfontsize">If you have question, please contact : </br>
        </br>
        <ul>
          <li><strong><em>Dr. Li Liu</em></strong> : li.liu@oulu.fi</li>
          </br>
          <li><strong><em >Dr. Peter Vajda</em></strong> : vajdap@fb.com </li>
          </br>
          <li><strong><em>Dr. Werner Bailer</em></strong> : werner.bailer@joanneum.at</li>
        </ul>
        </br>
      </div>
    </div>
  </div>
</div>
<div class="section footer">
  <div class="w-container">
    <div class="w-row">
      <div class="w-col w-col-3">
        <div class="logo-text footer">EDLCV 2020</div>
        <div class="social-icon-group"> <a href="#" class="social-icon w-inline-block"><img src="https://uploads-ssl.webflow.com/5abe7e9055d981785ad0393e/5abe7e9055d981965fd03963_facebook-icon.svg" alt=""/></a> <a href="#" class="social-icon w-inline-block"><img src="https://uploads-ssl.webflow.com/5abe7e9055d981785ad0393e/5abe7e9055d981167bd03965_twitter-icon.svg" alt=""/></a> <a href="#" class="social-icon w-inline-block"><img src="https://uploads-ssl.webflow.com/5abe7e9055d981785ad0393e/5abe7e9055d98147a9d03966_linkdin-icon-white.svg" alt=""/></a> <a href="#" class="social-icon w-inline-block"><img src="https://uploads-ssl.webflow.com/5abe7e9055d981785ad0393e/5abe7e9055d981a51dd0395b_email-icon-white.svg" alt=""/></a> </div>
      </div>
      <div class="w-col w-col-9"></div>
    </div>
  </div>
</div>
<script src="https://code.jquery.com/jquery-3.3.1.min.js" type="text/javascript" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><script src="https://uploads-ssl.webflow.com/5abe7e9055d981785ad0393e/js/webflow.2e96abf72.js" type="text/javascript"></script><!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->
</body>
</html>
