<!DOCTYPE html><!-- 让浏览器得知自己处理的是html -->
<html lang="en">

<!--对文本进行操作	-->
<!--a表示创建超链接，href是链接到的那个东西，target选择新开网页打开链接或者在原网页打开-->
<!--b表示粗体  em表示斜体  s表示删除线-->

<head>
<!--提供有关文档内容和标注信息-->
<meta charset="utf-8"/>
<title>EDLCV</title>
<!--有head必须有title-->
<meta content="width=device-width, initial-scale=1" name="viewport"/>
<meta content="Webflow" name="generator"/>
<link href=".\css\CVPR2020.css" rel="stylesheet" type="text/css"/>
<!--链接一个css--> 
<script src=".\css\CVPR2020.js" type="text/javascript"></script> 
<script type="text/javascript">WebFont.load({  google: {    families: ["Roboto:300,regular,500","Roboto Condensed:300,regular,700","Roboto Slab:300,regular,700","Arbutus Slab:regular"]  }});</script> 
<script type="text/javascript">!function(o,c){var n=c.documentElement,t=" w-mod-";n.className+=t+"js",("ontouchstart"in o||o.DocumentTouch&&c instanceof DocumentTouch)&&(n.className+=t+"touch")}(window,document);</script>
</head>

<body>
<div data-collapse="medium" data-animation="default" data-duration="400" class="navigation w-hidden-main w-nav">
  <div class="w-container"><a href="#" class="brand-link w-nav-brand">
    <div class="logo-text">EDLCV 2020</div>
    </a>
    <nav role="navigation" class="nav-menu w-nav-menu"> <a href="#topics" class="nav-link w-nav-link">Topics</a> <a href="#dates" class="nav-link w-nav-link">Dates</a> <a href="#speakers" class="nav-link w-nav-link">Speakers</a> <a href="#submission" class="nav-link w-nav-link">Submission</a> <a href="#program2" class="nav-link w-nav-link">Program</a> <a href="#awards" class="nav-link w-nav-link">Awards</a> <a href="#organizers" class="nav-link w-nav-link">Organizers</a> </nav>
    <div class="nav-link menu w-nav-button">
      <div class="w-icon-nav-menu"></div>
    </div>
  </div>
</div>
<div data-collapse="medium" data-animation="default" data-duration="400" class="navigation w-nav">
  <div class="w-container"><a href="#" class="brand-link w-nav-brand">
    <div class="logo-text">EDLCV 2020</div>
    </a>
    <nav role="navigation" class="nav-menu w-nav-menu"> 
		<a href="#dates" class="nav-link w-nav-link">Dates</a> 
		<a href="#topics" class="nav-link w-nav-link">Topics</a> 
		<a href="#speakers" class="nav-link w-nav-link">Speakers</a> 
		<a href="#program" class="nav-link w-nav-link">Program</a> 
		<a href="#session1" class="nav-link w-nav-link">Accepted Papers</a> 
		<a href="#submission2" class="nav-link w-nav-link">Submission</a> 
		<a href="#organizers" class="nav-link w-nav-link">Organizers</a> 
		<a href="#awards2" class="nav-link w-nav-link">Previous</a> </nav>
  </div>
</div>
<div data-animation="slide" data-duration="500" data-infinite="1" class="slider w-slider">
  <div class="w-slider-mask">
    <div class="slide _1 w-slide">
      <div class="w-container"></div>
    </div>
    <div class="slide _2 w-slide">
      <div class="w-container"></div>
    </div>
    <div class="slide _3 w-slide">
      <div class="w-container"></div>
    </div>
  </div>
  <div class="w-slider-arrow-left">
    <div class="w-icon-slider-left"></div>
  </div>
  <div class="w-slider-arrow-right">
    <div class="w-icon-slider-right"></div>
  </div>
  <div class="w-slider-nav w-round"></div>
</div>
<div class="section main w-hidden-small w-hidden-tiny">
  <div class="container w-container">
    <h1 class="main-heading">Joint Workshop on <br/>
      Efficient Deep Learning in Computer Vision</h1>
    <div class="text-block-7">June 15th, 2020 <br/>
      Seattle, Washington <br/>
      in conjunction with <a href="http://cvpr2020.thecvf.com/"  class="link">CVPR 2020</a></div>
  </div>
</div>
<div class="section main w-hidden-main w-hidden-medium">
  <div class="container w-container">
    <h1 class="main-heading">Joint Workshop on <br/>
      Efficient Deep Learning in Computer Vision <br/>
    </h1>
    <div class="text-block-7">June 15th, 2020<br/>
      Seattle, Washington <br/>
      in conjunction with <a href="http://cvpr2020.thecvf.com/" class="link">CVPR 2020</a></div>
  </div>
</div>
<div class="section-10 w-hidden-small w-hidden-tiny">
  <div class="w-container">
    <div>
      <div class="text-block-8">Computer Vision has a long history of academic research, and recent advances in deep learning have provided significant improvements in the ability to understand visual content. As a result of these research advances on problems such as object classification, object detection, and image segmentation, there has been a rapid increase in the adoption of Computer Vision in industry; however, mainstream Computer Vision research has given little consideration to speed or computation time, and even less to constraints such as power/energy, memory footprint and model size. The workshop has three main goals on solving and discussing efficiency in Computer Vision: <br/>
        <br/>
        First, the workshop aims to create a venue for a consideration of the new generation of problems that arise as Computer Vision meets mobile and AR/VR systems constraints, to bring together researchers, educators and practitioners who are interested in techniques as well as applications of compact, efficient neural network representations. The workshop discussions will establish close connection between researchers in machine learning and computer vision communities and engineers in industry, and to benefit both academic researchers as well as industrial practitioners. <br/>
        <br/>
        Second, the workshop aims at reproducibility and comparability of methods for compact and efficient neural network representations, and on-device machine learning. Thus a set of benchmarking tasks (image classification, visual question answering) will be provided together with defined data sets, in order to compare the performance of neural network compression methods on the same networks. Submissions are encouraged (but not required) to use these tasks and data sets in their work. Also, contributors are encouraged to make their code available. <br/>
        <br/>
        Third, the workshop aims to discuss the next steps in developing efficient feature representations from three aspects: energy efficient, label efficient, and sample efficient. Despite DNNs are brain-inspired and can achieve or even surpass human-level performance on a variety of challenging computer vision tasks, they continue to trail humans’ abilities in many aspects, such as high energy-efficiency and the ability to perform low-shot learning (learning novel concepts from very few examples). Therefore, the next generation of feature representation and learning techniques should aim to tackle recognition tasks with significantly reduced computational complexity, using as little training data as people need, and to generalize to a range of different tasks beyond the one task the model was trained on. </div>
    </div>
  </div>
</div>
<div class="section-2 w-hidden-main w-hidden-medium">
  <div class="w-container">
    <div>
      <div class="text-block-12 w-hidden-main w-hidden-medium">Computer Vision has a long history of academic research, and recent advances in deep learning have provided significant improvements in the ability to understand visual content. As a result of these research advances on problems such as object classification, object detection, and image segmentation, there has been a rapid increase in the adoption of Computer Vision in industry; however, mainstream Computer Vision research has given little consideration to speed or computation time, and even less to constraints such as power/energy, memory footprint and model size. The workshop has three main goals on solving and discussing efficiency in Computer Vision: <br/>
        First, the workshop aims to create a venue for a consideration of the new generation of problems that arise as Computer Vision meets mobile and AR/VR systems constraints, to bring together researchers, educators and practitioners who are interested in techniques as well as applications of compact, efficient neural network representations. The workshop discussions will establish close connection between researchers in machine learning and computer vision communities and engineers in industry, and to benefit both academic researchers as well as industrial practitioners. <br/>
        Second, the workshop aims at reproducibility and comparability of methods for compact and efficient neural network representations, and on-device machine learning. Thus a set of benchmarking tasks (image classification, visual question answering) will be provided together with defined data sets, in order to compare the performance of neural network compression methods on the same networks. Submissions are encouraged (but not required) to use these tasks and data sets in their work. Also, contributors are encouraged to make their code available. <br/>
        Third, the workshop aims to discuss the next steps in developing efficient feature representations from three aspects: energy efficient, label efficient, and sample efficient. Despite DNNs are brain-inspired and can achieve or even surpass human-level performance on a variety of challenging computer vision tasks, they continue to trail humans’ abilities in many aspects, such as high energy-efficiency and the ability to perform low-shot learning (learning novel concepts from very few examples). Therefore, the next generation of feature representation and learning techniques should aim to tackle recognition tasks with significantly reduced computational complexity, using as little training data as people need, and to generalize to a range of different tasks beyond the one task the model was trained on. <br/>
      </div>
    </div>
  </div>
</div>
<div id="dates" class="section purple w-hidden-small w-hidden-tiny">
  <div class="w-container">
    <h2 class="heading-3">Important Dates</h2>
    <div>
      <div class="w-row">
        <div class="column-2 w-col w-col-5">
          <div class="text-block-9"><strong class="bold-text-3">Paper Submission Deadline:</strong></div>
          <div><strong class="bold-text-4">Notification to authors:</strong></div>
          <div class="text-block-10"><strong class="bold-text-5">Camera ready deadline:</strong></div>
          <div><strong class="bold-text-6">Workshop:</strong></div>
        </div>
        <div class="column-3 w-clearfix w-col w-col-7">
          <div class="text-block-5"><strong class="bold-text-8">March 25, 2020 pst</strong></div>
          <div class="text-block-6 w-clearfix"><strong class="bold-text-9">April 12, 2020 pst</strong></div>
          <div><strong class="bold-text-2">April 19, 2020 pst</strong></div>
          <div><strong class="bold-text-2">June 15, 2020 (Full Day)</strong></div>
        </div>
      </div>
    </div>
  </div>
</div>
<div id="dates" class="section purple w-hidden-main w-hidden-medium">
  <div class="w-container">
    <h2 class="heading-3">Important Dates</h2>
    <div class="row-3 w-row">
      <div class="w-col w-col-6">
        <div><strong class="bold-text-3">Paper Submission Deadline:</strong></div>
        <div><strong class="bold-text-4">Notification to authors:</strong></div>
        <div><strong class="bold-text-5">Camera ready deadline:</strong></div>
        <div><strong class="bold-text-6">Workshop:</strong></div>
      </div>
      <div class="w-col w-col-6">
        <div class="text-block-16"><strong class="bold-text-12"> March 25, 2020 pst</strong></div>
        <div><strong class="bold-text-3">April 12, 2020 pst</strong></div>
        <div><strong class="bold-text-3">April 19, 2020 pst</strong></div>
        <div><strong class="bold-text-3">June 15, 2020 (Full Day)</strong></div>
      </div>
    </div>
  </div>
</div>
<div id="topics" class="section-8">
  <div class="container-4 w-container">
    <h2 class="heading-11">Topics</h2>
    <div class="section-subtitle">
      <ul>
        <li><strong>Efficient Neural Network and Architecture Search</strong></li>
        <ul>
          <li>Compact and efficient neural network architecture for mobile and AR/VR devices</li>
          <li>Hardware (latency, energy) aware neural network architectures search, targeted for mobile and AR/VR devices</li>
          <li>Efficient architecture search algorithm for different vision tasks (detection, segmentation etc.)</li>
          <li>Optimization for Latency, Accuracy and Memory usage, as motivated by embedded devices</li>
        </ul>
        <li><strong>Neural Network Compression</strong></li>
        <ul>
          <li>Model compression (sparsification, binarization, quantization, pruning, thresholding and coding etc.) for efficient inference with deep networks and other ML models</li>
          <li>Scalable compression techniques that can cope with large amounts of data and/or large neural networks (<em>e.g.</em>, not requiring access to complete datasets for hyperparameter tuning and/or retraining)</li>
          <li>Hashing (Binary) Codes Learning</li>
        </ul>
        <li><strong>Low-bit Quantization Network and Hardware Accelerators</strong></li>
        <ul>
          <li>Investigations into the processor architectures (CPU vs GPU vs DSP) that best support mobile applications</li>
          <li>Hardware accelerators to support Computer Vision on mobile and AR/VR platforms</li>
          <li>Low-precision training/inference & acceleration of deep neural networks on mobile devices</li>
        </ul>
        <li><strong>Dataset and benchmark</strong></li>
        <ul>
          <li>Open datasets and test environments for benchmarking inference with efficient DNN representations</li>
          <li>Metrics for evaluating the performance of efficient DNN representations</li>
          <li>Methods for comparing efficient DNN inference across platforms and tasks</li>
        </ul>
        <li><strong>Label/sample/feature efficient learning</strong></li>
        <ul>
          <li>Label Efficient Feature Representation Learning Methods, <em>e.g.</em> Unsupervised Learning, Domain Adaptation, Weakly Supervised Learning and SelfSupervised Learning Approaches</li>
          <li>Sample Efficient Feature Learning Methods, <em>e.g.</em> Meta Learning</li>
          <li>Low Shot learning Techniques</li>
          <li>New Applications, <em>e.g.</em> Medical Domain</li>
        </ul>
        <li><strong>Mobile and AR/VR Applications</strong></li>
        <ul>
          <li>Novel mobile and AR/VR applications using Computer Vision such as image processing (<em>e.g.</em> style transfer, body tracking, face tracking) and augmented reality</li>
          <li>Learning efficient deep neural networks under memory and computation constraints for on-device applications</li>
        </ul>
      </ul>
    </div>
  </div>
</div>
<div id="speakers" class="section-3 w-hidden-small w-hidden-tiny">
  <div class="w-container">
    <h2 class="heading-5">Keynote Speakers</h2>
    <div>
      <div class="w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/philip.png" width="64" alt="" class="person"/>
          <h1 class="heading">Philip Torr<br/>
            University of Oxford, <br/>
            the United Kingdom</h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-2">Title: <strong>TBD</strong><br/>
            <br/>
            Biography: Philip Torr did his PhD (DPhil) at the Robotics Research Group of the University of Oxford under Professor David Murray of the Active Vision Group. He worked for another three years at Oxford as a research fellow, and is still maintains close contact as visiting fellow there. He left Oxford to work for six years as a research scientist for Microsoft Research, first in Redmond USA in the Vision Technology Group, then in Cambridge UK founding the vision side of the Machine learning and perception group. He then became a Professor in  in Computer Vision and Machine Learning at Oxford Brookes University, where he has brought in over one million pounds in grants for which he is PI. Recently in 2013 he returned to Oxford as full professor where he has established the Torr Vision group and has brought in over five million pounds of funding. Philip Torr won several awards including the Marr prize (the highest honour in vision) in 1998.  He is a Royal Society Wolfson Research Merit Award Holder. More recently he has been awarded best science paper at BMVC 2010 and ECCV 2010. He was involved in the algorithm design for Boujou released by 2D3. Boujou has won a clutch of industry awards, including Computer Graphics World Innovation Award, IABM Peter Wayne Award, and CATS Award for Innovation, and a technical EMMY. He then worked closely with this Oxford based company as well as other companies such as Sony on the Wonderbook project. He is a director of new Oxford based spin out OxSight.</div>
        </div>
      </div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="http://niclane.org/" class="w-inline-block"><img src="pictures/nic.png" width="64" alt="" class="person"/>
          <h1 class="heading">Prof. Nic Lane<br/>
            University of Oxford, <br/>
            the United Kingdom</h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3">Title: <strong>TBD <!--(--></strong><!--<a target="_blank" href="" class="link-13"><strong>Slides</strong></a><strong>)</strong><br/>--> 
            <br/>
            <br/>
            Biography: Nic Lane received his Ph.D. degree in 2011 from Dartmouth College, Hanover, New Hampshire. He is an associate professor in the Computer Science Department at the University of Oxford, United Kingdom. He is an experimentalist and likes to build prototype next-generation wearable and embedded-sensing devices based on well-founded computational models. His work has received multiple best paper awards, including one from the ACM/IEEE Conference on Information Processing in Sensor Networks 2017 and two from ACM UbiComp in 2012 and 2015, respectively. (Based on document published on 5 September 2018).</div>
        </div>
      </div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/diana.png" width="64" alt="" class="person"/>
          <h1 class="heading">Prof. Diana Marculescu<br/>
            CMU</h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3">Title: <strong>TBD <!--(--></strong><!--<a target="_blank" href="" class="link-13"><strong>Slides</strong></a><strong>)</strong><br/>--> 
            <br/>
            <br/>
            Biography: Diana Marculescu received a degree in computer science from Politehnica University of Bucharest, Romania, in 1991, and a Ph.D. in computer engineering from the University of Southern California in 1998. From 2014 to 2018, she served as Associate Department Head for Academic Affairs in Electrical and Computer Engineering, and, from 2015 to 2019, was the founding director of the College of Engineering Center for Faculty Success. In 2019, the Cockrell School of Engineering at the University of Texas at Austin, had named her New Chair of Electrical and Computer Engineering.</div>
        </div>
      </div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/Songhan.png" width="64" alt="" class="person"/>
          <h1 class="heading">Song Han<br/>
            MIT, the United States</h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3">Title: <strong>TBD<!--(--></strong><!--<a target="_blank" href="" class="link-11"><strong>Slides</strong></a><strong>)<br/>--> 
            <br/>
            <br/>
            </strong>Biography: Song Han is an assistant professor at MIT EECS. Dr. Han received the Ph.D. degree in Electrical Engineering from Stanford advised by Prof. Bill Dally. Dr. Han’s research focuses on efficient deep learning computing. He proposed “Deep Compression” and “Efficient Inference Engine” that impacted the industry. His work received the best paper award in ICLR’16 and FPGA’17. He is the co-founder and chief scientist of DeePhi Tech (a leading efficient deep learning solution provider), which was acquired by Xilinx. The pruning, compression and acceleration techniques have been integrated into products.</div>
        </div>
      </div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/bill.jpg" width="64" alt="" class="person"/>
          <h1 class="heading">Prof. Bill Dally<br/>
            Stanford University, <br/>
            the United States </h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3">Title: <strong>TBD<!--(--></strong><!--<a target="_blank" href="" class="link-13"><strong>Slides</strong></a><strong>)</strong><br/>--> 
            <br/>
            <br/>
            Biography: Bill Dally develops efficient hardware for demanding information processing problems and sustainable energy systems. His current projects include domain-specific accelerators for deep learning, bioinformatics, and SAT solving; redesigning memory systems for the data center; developing efficient methods for video perception; and developing efficient sustainable energy systems. His research involves demonstrating novel concepts with working systems. Previous systems include the MARS Hardware Accelerator, the Torus Routing Chip, the J-Machine, M-Machine, the Reliable Router, the Imagine signal and image processor, the Merrimac supercomputer, and the ELM embedded processor. His work on stream processing led to GPU computing. His group has pioneered techniques including fast capability-based addressing, processor coupling, virtual channel flow control, wormhole routing, link-level retry, message-driven processing, deadlock-free routing, pruning neural networks, and quantizing neural networks.</div>
        </div>
      </div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/chelsea.png" width="64" alt="" class="person"/>
          <h1 class="heading">Chelsea Finn<br/>
            Stanford University, <br/>
            the United States </h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3">Title: <strong>TBD<!--(--></strong><!--<a target="_blank" href="" class="link-13"><strong>Slides</strong></a><strong>)</strong><br/>--> 
            <br/>
            <br/>
            Biography: Chelsea Finn completed her Ph.D. in computer science at UC Berkeley and her B.S. in electrical engineering and computer science at MIT. Now she is a research scientist at Google Brain, a post-doc at Berkeley AI Research Lab (BAIR), and an acting assistant professor at Stanford. She will join the Stanford Computer Science faculty full time, starting in Fall 2019. She is interested in how algorithms can enable machines to acquire more general notions of intelligence through learning and interaction, allowing them to autonomously learn a variety of complex sensorimotor skills in real-world settings. This includes learning deep representations for representing complex skills from raw sensory inputs, enabling machines to learn through interaction without human supervision, and allowing systems to build upon what they’ve learned previously to acquire new capabilities with small amounts of experience.</div>
        </div>
      </div>
    </div>
  </div>
</div>
<div id="speakers" class="section-3 w-hidden-main w-hidden-medium">
  <div class="w-container">
    <h2 class="heading-10">Keynote Speakers</h2>
    <div>
      <div class="w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/philip.png" width="64" alt="" class="person"/>
          <h1 class="heading">Philip Torr<br/>
            University of Oxford, <br/>
            the United Kingdom</h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-2">Title: <strong>TBD</strong><br/>
            <br/>
            <br/>
            Biography: Philip Torr did his PhD (DPhil) at the Robotics Research Group of the University of Oxford under Professor David Murray of the Active Vision Group. He worked for another three years at Oxford as a research fellow, and is still maintains close contact as visiting fellow there. He left Oxford to work for six years as a research scientist for Microsoft Research, first in Redmond USA in the Vision Technology Group, then in Cambridge UK founding the vision side of the Machine learning and perception group. He then became a Professor in  in Computer Vision and Machine Learning at Oxford Brookes University, where he has brought in over one million pounds in grants for which he is PI. Recently in 2013 he returned to Oxford as full professor where he has established the Torr Vision group and has brought in over five million pounds of funding. Philip Torr won several awards including the Marr prize (the highest honour in vision) in 1998.  He is a Royal Society Wolfson Research Merit Award Holder. More recently he has been awarded best science paper at BMVC 2010 and ECCV 2010. He was involved in the algorithm design for Boujou released by 2D3. Boujou has won a clutch of industry awards, including Computer Graphics World Innovation Award, IABM Peter Wayne Award, and CATS Award for Innovation, and a technical EMMY. He then worked closely with this Oxford based company as well as other companies such as Sony on the Wonderbook project. He is a director of new Oxford based spin out OxSight.</div>
        </div>
      </div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="http://niclane.org/" class="w-inline-block"><img src="pictures/nic.png" width="64" alt="" class="person"/>
          <h1 class="heading">Prof. Nic Lane<br/>
            University of Oxford, <br/>
            the United Kingdom</h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3">Title: <strong>TBD <!--(--></strong><!--<a target="_blank" href="" class="link-13"><strong>Slides</strong></a><strong>)</strong><br/>--> 
            <br/>
            <br/>
            Biography: Nic Lane received his Ph.D. degree in 2011 from Dartmouth College, Hanover, New Hampshire. He is an associate professor in the Computer Science Department at the University of Oxford, United Kingdom. He is an experimentalist and likes to build prototype next-generation wearable and embedded-sensing devices based on well-founded computational models. His work has received multiple best paper awards, including one from the ACM/IEEE Conference on Information Processing in Sensor Networks 2017 and two from ACM UbiComp in 2012 and 2015, respectively. (Based on document published on 5 September 2018).</div>
        </div>
      </div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/diana.png" width="64" alt="" class="person"/>
          <h1 class="heading">Prof. Diana Marculescu<br/>
            CMU</h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3">Title: <strong>TBD <!--(--></strong><!--<a target="_blank" href="" class="link-13"><strong>Slides</strong></a><strong>)</strong><br/>--> 
            <br/>
            <br/>
            Biography: Diana Marculescu received a degree in computer science from Politehnica University of Bucharest, Romania, in 1991, and a Ph.D. in computer engineering from the University of Southern California in 1998. From 2014 to 2018, she served as Associate Department Head for Academic Affairs in Electrical and Computer Engineering, and, from 2015 to 2019, was the founding director of the College of Engineering Center for Faculty Success. In 2019, the Cockrell School of Engineering at the University of Texas at Austin, had named her New Chair of Electrical and Computer Engineering.</div>
        </div>
      </div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/Songhan.png" width="64" alt="" class="person"/>
          <h1 class="heading">Song Han<br/>
            MIT, the United States</h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3">Title: <strong>TBD<!--(--></strong><!--<a target="_blank" href="" class="link-11"><strong>Slides</strong></a><strong>)<br/>--> 
            <br/>
            <br/>
            </strong>Biography: Song Han is an assistant professor at MIT EECS. Dr. Han received the Ph.D. degree in Electrical Engineering from Stanford advised by Prof. Bill Dally. Dr. Han’s research focuses on efficient deep learning computing. He proposed “Deep Compression” and “Efficient Inference Engine” that impacted the industry. His work received the best paper award in ICLR’16 and FPGA’17. He is the co-founder and chief scientist of DeePhi Tech (a leading efficient deep learning solution provider), which was acquired by Xilinx. The pruning, compression and acceleration techniques have been integrated into products.</div>
        </div>
      </div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/bill.jpg" width="64" alt="" class="person"/>
          <h1 class="heading">Prof. Bill Dally<br/>
            Stanford University, <br/>
            the United States </h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3">Title: <strong>TBD <!--(--></strong><!--<a target="_blank" href="" class="link-13"><strong>Slides</strong></a><strong>)</strong><br/>--> 
            <br/>
            <br/>
            Biography: Bill Dally develops efficient hardware for demanding information processing problems and sustainable energy systems. His current projects include domain-specific accelerators for deep learning, bioinformatics, and SAT solving; redesigning memory systems for the data center; developing efficient methods for video perception; and developing efficient sustainable energy systems. His research involves demonstrating novel concepts with working systems. Previous systems include the MARS Hardware Accelerator, the Torus Routing Chip, the J-Machine, M-Machine, the Reliable Router, the Imagine signal and image processor, the Merrimac supercomputer, and the ELM embedded processor. His work on stream processing led to GPU computing. His group has pioneered techniques including fast capability-based addressing, processor coupling, virtual channel flow control, wormhole routing, link-level retry, message-driven processing, deadlock-free routing, pruning neural networks, and quantizing neural networks.</div>
        </div>
      </div>
      <div class="row w-row">
        <div class="column w-col w-col-3"><a href="" class="w-inline-block"><img src="pictures/chelsea.png" width="64" alt="" class="person"/>
          <h1 class="heading">Chelsea Finn<br/>
            Stanford University, <br/>
            the United States </h1>
          </a></div>
        <div class="w-col w-col-9">
          <div class="text-block-3"> Title: <strong>TBD <!--(--></strong><!--<a target="_blank" href="" class="link-13"><strong>Slides</strong></a><strong>)</strong><br/>--> 
            <br/>
            <br/>
            Biography: Chelsea Finn completed her Ph.D. in computer science at UC Berkeley and her B.S. in electrical engineering and computer science at MIT. Now she is a research scientist at Google Brain, a post-doc at Berkeley AI Research Lab (BAIR), and an acting assistant professor at Stanford. She will join the Stanford Computer Science faculty full time, starting in Fall 2019. She is interested in how algorithms can enable machines to acquire more general notions of intelligence through learning and interaction, allowing them to autonomously learn a variety of complex sensorimotor skills in real-world settings. This includes learning deep representations for representing complex skills from raw sensory inputs, enabling machines to learn through interaction without human supervision, and allowing systems to build upon what they’ve learned previously to acquire new capabilities with small amounts of experience.</div>
        </div>
      </div>
    </div>
  </div>
</div>
<div id="program" class="section w-hidden-small w-hidden-tiny">
  <div class="w-container">
    <h2 id="program" class="heading-6">Program (Tentative)</h2>
    <div class="text-block-17"> (Location: <a href="" target="_blank" class="link-8"> TBD</a> <a href="" target="_blank" class="link-9"> </a>) </div>
    <div>
      <div class="html-embed w-embed">
        <table  border="1" width="900" height="650" align="center">
          <tr>
            <th font size="1" bgcolor="#808080" align="center" > <font color="white" > <font size="4"> Time</font> </th>
            <th bgcolor="#808080" align="center" > <font color="white"><font size="4">Event</font> </th>
          </tr>
          <tr align="center" >
            <td><font size="3">8:50 - 9:00 </font></td>
            <td><font size="3">Welcome by organizers</font></td>
          </tr>
          <tr align="center" >
            <td><font size="3">9:00 - 9:30 </font></td>
            <td><b><font size="3">Invited talk: Prof. Philip Torr (Oxford University)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="3">9:30 - 10:00 </font></td>
            <td><b><font size="3">Invited talk: Prof. Nic Lane (Oxford University)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="3">10:00 - 10:30 </font></td>
            <td><font size="3">Coffee break</font></td>
          </tr>
          <tr align="center" >
            <td><font size="3">10:30 - 11:00</font></td>
            <td><b><font size="3">Oral Session 1 (3  presentations: 10min each)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="3">11:00 - 11:30</font></td>
            <td><b><font size="3">Keynote talk: Prof. Song Han (MIT)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="3">11:30 - 12:00 </font></td>
            <td><b><font size="3">Invited talk: Prof. Diana Marculescu (CMU)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="3">12:00 - 12:30 </font></td>
            <td><b><font size="3">Oral Session 2 (3  presentations: 10min each)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="3">12:30 - 13:30 </font></td>
            <td><font size="3"> Lunch break</font></td>
          </tr>
          <tr align="center" >
            <td><font size="3">13:30 - 14:00 </font></td>
            <td><b><font size="3">Keynote talk: Prof. Bill Dally (Stanford)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="3">14:00 - 14:30 </font></td>
            <td><b><font size="3">Invited talk: Prof. Chelsea Finn (Stanford)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="3">14:30 - 15:00 </font></td>
            <td><b><font size="3">Oral Session 3 (3  presentations: 10min each)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="3">15:00 - 16:00 </font></td>
            <td><b><font size="3">Poster session by paper submission</font></td>
          </tr>
          <tr align="center" >
            <td><font size="3">16:00 - 17:30 </font></td>
            <td><b><font size="3">Panel presentations and discussion on Efficient deep learning algorithms. </br/>Moderator: Luc Van Gool</font></td>
          </tr>
          <tr align="center" >
            <td><font size="3">17:30 - 17:45 </font></td>
            <td><b><font size="3">Closing awards for best paper and best poster</font></td>
          </tr>
        </table>
      </div>
    </div>
  </div>
</div>

<!--table表示创建表格  tr表示一行中的单元格 td表示单元格  th表示标题--> 
<!--thead表示表头  tbody表示表的内容  tfoot表示表脚--> 
<!--br换行 没有/br--> 
<!--colspan表示合并行单元格  rowspan表示合并列单元格-->

<div id="program2" class="section w-hidden-main w-hidden-medium">
  <div class="w-container">
    <h2 id="program" class="heading-8">Program (Tentative)</h2>
    <div class="text-block-17"> (Location: <a href="" target="_blank" class="link-8"> TBD </a> <a href="" target="_blank" class="link-9"> </a>) </div>
    <div>
      <div class="html-embed-2 w-embed">
        <table  border="1" width="300" align="center" >
          <tr>
            <th font size="1" bgcolor="#808080" align="center" > <font color="white" > <font size="1"> Time</font> </th>
            <th bgcolor="#808080" align="center" > <font color="white"><font size="1">Event</font> </th>
          </tr>
          <tr align="center" >
            <td><font size="1">8:50 - 9:00 </font></td>
            <td><font size="1">Welcome by organizers</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">9:00 - 9:30 </font></td>
            <td><b><font size="1">Invited talk: Prof. Philip Torr (Oxford University)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">9:30 - 10:00 </font></td>
            <td><b><font size="1">Invited talk: Prof. Nic Lane (Oxford University)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">10:00 - 10:30 </font></td>
            <td><font size="1">Coffee break</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">10:30 - 11:00</font></td>
            <td><b><font size="1">Oral Session 1 (3  presentations: 10min each)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">11:00 - 11:30</font></td>
            <td><b><font size="1">Keynote talk: Prof. Song Han (MIT)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">11:30 - 12:00 </font></td>
            <td><b><font size="1">Invited talk: Prof. Diana Marculescu (CMU)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">12:00 - 12:30 </font></td>
            <td><b><font size="1">Oral Session 2 (3  presentations: 10min each)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">12:30 - 13:30 </font></td>
            <td><font size="1"> Lunch break</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">13:30 - 14:00 </font></td>
            <td><b><font size="1">Keynote talk: Prof. Bill Dally (Stanford)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">14:00 - 14:30 </font></td>
            <td><b><font size="1">Invited talk: Prof. Chelsea Finn (Stanford)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">14:30 - 15:00 </font></td>
            <td><b><font size="1">Oral Session 3 (3  presentations: 10min each)</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">15:00 - 16:00 </font></td>
            <td><b><font size="1">Poster session by paper submission</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">16:00 - 17:30 </font></td>
            <td><b><font size="1">Panel presentations and discussion on Efficient deep learning algorithms. Moderator: Luc Van Gool</font></td>
          </tr>
          <tr align="center" >
            <td><font size="1">17:30 - 17:45 </font></td>
            <td><b><font size="1">Closing awards for best paper and best poster</font></td>
          </tr>
        </table>
      </div>
    </div>
  </div>
</div>
<div id="session1" class="section-9">
  <div class="w-container">
    <h1 class="heading-5" align="center">Accepted Papers(TBD)</h1>
  </div>
</div>
<div class="section-6 w-hidden-small w-hidden-tiny">
  <div class="w-container">
    <div>
      <h2 class="heading-7" align="center">
      Awards
      </h2>
      <div class="text-block-14">EDLCV 2020 will announce one <strong><em class="italic-text-5">Best Paper Award</em></strong> and one <strong><em class="italic-text-7">Best</em></strong><span> <strong><em class="italic-text-6">Paper Honorable Mention Award</em></strong></span>, fully sponsored by the <a href="https://www.inceptioniai.org/"><strong><em class="italic-text-2">Inception Institute of Artificial Intelligence</em></strong></a>:<br/>
        <br/>
        ※ EDLCV 2020 Best Paper Award ($1,500)<br/>
        <strong></strong><em><br/>
        ‍<br/>
        ※ </em>EDLCV 2020 Best Paper Honorable Mention Award ($500)<br/>
        <strong></strong><em><br/>
        ‍</em></div>
    </div>
  </div>
</div>
<div class="section-6 w-hidden-main w-hidden-medium">
  <div class="w-container">
    <div class="div-block">
      <h2 class="heading-7" align="center">Awards</h2>
      <div class="text-block-14">EDLCV 2020 will announce one <strong><em class="italic-text-5">Best Paper Award</em></strong> and one <strong><em class="italic-text-7">Best</em></strong><span> <strong><em class="italic-text-6">Paper Honorable Mention Award</em></strong></span>, fully sponsored by the <a href="https://www.inceptioniai.org/"><strong><em class="italic-text-2">Inception Institute of Artificial Intelligence</em></strong></a>:<br/>
        <br/>
        ※ EDLCV 2020 Best Paper Award ($1,500)<br/>
        <strong></strong><em><br/>
        ‍<br/>
        ※ </em>EDLCV 2020 Best Paper Honorable Mention Award ($500)<br/>
        <strong></strong><em><br/>
        ‍</em></div>
    </div>
  </div>
</div>
<div id="submission" class="section-7 w-hidden-main w-hidden-medium w-hidden-small w-hidden-tiny">
  <div id="awards" class="w-container">
    <div>
      <h2 class="heading-12">Submission</h2>
      <div class="text-block-15">
        <p>All submissions will be handled electronically via the workshop’s CMT Website.  Click the following link to go to the submission site: <a href="https://cmt3.research.microsoft.com/EDLCV2020/" class="newfont">https://cmt3.research.microsoft.com/EDLCV2020/</a></p>
        
		  <p>Papers should describe original and unpublished work about the related topics. Each paper will receive double blind reviews, moderated by the workshop chairs. Authors should take into account the following:</p>
		  <ul>
        <li>All papers must be written and presented in English.</li>
        <li>All papers must be submitted in PDF format. The workshop paper format guidelines are the same as the <a href="http://cvpr2020.thecvf.com/submission/main-conference/author-guidelines#submission-guidelines" class="newfont">Main Conference papers</a></li>
        <li>The maximum paper length is 8 pages (excluding references). Note that shorter submissions are also welcome.</li>
        <li>The accepted papers will be published in CVF open access as well as in IEEE Xplore.</li>
		  </ul>
		  
      </div>
    </div>
  </div>
</div>
<div id="submission2" class="section-7">
  <div id="awards" class="w-container">
    <div>
      <h2 class="heading-12">Submission</h2>
      <div class="text-block-15">
        <p>All submissions will be handled electronically via the workshop’s CMT Website.  Click the following link to go to the submission site: <a href="https://cmt3.research.microsoft.com/EDLCV2020/" class="newfont">https://cmt3.research.microsoft.com/EDLCV2020/</a></p>
        <p>Papers should describe original and unpublished work about the related topics. Each paper will receive double blind reviews, moderated by the workshop chairs. Authors should take into account the following:</p>
        <ul>
        <li>All papers must be written and presented in English.</li>
        <li>All papers must be submitted in PDF format. The workshop paper format guidelines are the same as the <a href="http://cvpr2020.thecvf.com/submission/main-conference/author-guidelines#submission-guidelines" class="newfont">Main Conference papers</a></li>
        <li>The maximum paper length is 8 pages (excluding references). Note that shorter submissions are also welcome.</li>
        <li>The accepted papers will be published in CVF open access as well as in IEEE Xplore.</li>
		  </ul>
		  
      </div>
    </div>
  </div>
</div>
<div id="organizers" class="section">
  <div class="w-container">
    <h2 class="heading-13">Organizers</h2>
    <div class="row-4 w-row">
      <table>
        <tr>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="http://www.ee.oulu.fi/~lili/LiLiuHomepage.html" class="w-inline-block"><img src="pictures/li.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Li Liu<br/>
            University of Oulu &amp; NUDT</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://homes.esat.kuleuven.be/~yliu/" class="w-inline-block"><img src="pictures/yuliu.png" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Yu Liu<br/>
            PSI group of KU Leuven</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://wlouyang.github.io/" class="w-inline-block"><img src="pictures/wanli.png" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Wanli Ouyang<br/>
            Univeristy of Sydney</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="http://ivg.au.tsinghua.edu.cn/Jiwen_Lu/" class="w-inline-block"><img src="pictures/jiwen.png" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Jiwen Lu<br/>
            Tsinghua University</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://scholar.google.com/citations?user=bjEpXBoAAAAJ&amp;hl=en" class="w-inline-block"><img src="pictures/matti.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Prof. Matti Pietikäinen<br/>
            University of Oulu</h1>
          </a>
          </td>
        </tr>
        <tr>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://www.vision.ee.ethz.ch/en/members/get_member.cgi?id=1" class="w-inline-block"><img src="pictures/luc.png" width="64" alt="" class="person"/>
          <h1 class="heading-2">Prof. Luc Van Gool<br/>
            ETH Zurich</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://research.fb.com/people/vajda-peter/" class="w-inline-block"><img src="pictures/peter.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Peter Vajda<br/>
            Research Scientist Manager <br/>
            at Facebook</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://research.fb.com/people/zhang-peizhao/" class="w-inline-block"><img src="pictures/zhang.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Peizhao Zhang<br/>
            Research Scientist <br/>
            at Facebook</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://research.fb.com/people/zhang-peizhao/" class="w-inline-block"><img src="pictures/pete.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Pete Warden<br/>
            Staff Research Engineer <br/>
            at Google</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://people.eecs.berkeley.edu/~keutzer/" class="w-inline-block"><img src="pictures/kurt.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Prof. Kurt Keutzer<br/>
            UC Berkeley</h1>
          </a>
          </td>
        </tr>
        <tr>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://www.jonathandekhtiar.eu/" class="w-inline-block"><img src="pictures/jonathan.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Jonathan Dekhtiar<br/>
            Deep Learning Engineer <br/>
            at Nvidia</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="http://iphome.hhi.de/samek/" class="w-inline-block"><img src="pictures/wojciech.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Wojciech Samek<br/>
            Fraunhofer Heinrich Hertz Institute</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://eiclab.net/" class="w-inline-block"><img src="pictures/lin.png" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Yingyan Lin<br/>
            Rice University</h1>
          </a>
          </td>
          <td align="center" valign="top" width="200" height="250" valign="top">
          <a href="https://www.joanneum.at/digital/das-institut/team/detail/employee/bailer/" class="w-inline-block"><img src="pictures/werner.jpg" width="64" alt="" class="person"/>
          <h1 class="heading-2">Dr. Werner Bailer<br/>
            Joanneum Research</h1>
          </a>
          </td>
        </tr>
      </table>
    </div>
  </div>
</div>
<div id="awards" class="section footer w-hidden-main w-hidden-medium">
  <div id="awards" class="container-2 w-container">
    <h2 class="heading-5">Previous EDLCV Workshop</h2>
    <div class="text-block-2 link">
      <ul>
		  <li> <a href="http://www.ee.oulu.fi/~lili/CEFRLatICCV2019.html" class="newfont">4<sup>th</sup> CEFRL Workshop in conjunction with ICCV 2019</a></li>
        <li> <a href="http://www.ee.oulu.fi/~lili/CEFRLatCVPR2019.html" class="newfont">3<sup>rd</sup> CEFRL Workshop in conjunction with CVPR 2019</a></li>
        <li> <a href="https://cefrl.webflow.io/" class="newfont">2<sup>nd</sup> CEFRL Workshop in conjunction with ECCV 2018</a></li>
        <li> <a href="http://www.ee.oulu.fi/~lili/ICCVW2017.html" class="newfont">1<sup>st</sup> CEFRL Workshop in conjunction with ICCV 2017</a></li>
		<li><a href="https://icml.cc/Conferences/2019/Schedule?showEvent=3520" class="newfont">ICML 2019: Joint Workshop on On-Device Machine Learning and Compact Deep Neural Network Representations (ODML-CDNNR)</a></li>
        <li> <a href="https://nips.cc/Conferences/2018/Schedule?showEvent=10941" class="newfont">NIPS 2018: workshop on Compact Deep Neural Networks with industrial applications</a></li>
        <li><a href="https://sites.google.com/view/ecv2019/" class="newfont">2<sup>nd</sup> Efficient Deep Learning for Computer Vision
          in conjunction with CVPR 2019</a></li>
        <li><a href="https://sites.google.com/view/ecv2018/" class="newfont">1<sup>st</sup> Efficient Deep Learning for Computer Vision
          in conjunction with CVPR 2018</a></li>
		  </ul>
    </div>
  </div>
</div>
<div id="awards2" class="section footer w-hidden-small w-hidden-tiny">
  <div id="awards" class="w-container">
    <div class="w-container">
      <h2 class="heading-5">Previous EDLCV Workshop</h2>
      <div class="text-block-2 link">
		  <ul>
		  <li> <a href="http://www.ee.oulu.fi/~lili/CEFRLatICCV2019.html" class="newfont">4<sup>th</sup> CEFRL Workshop in conjunction with ICCV 2019</a></li>
        <li> <a href="http://www.ee.oulu.fi/~lili/CEFRLatCVPR2019.html" class="newfont">3<sup>rd</sup> CEFRL Workshop in conjunction with CVPR 2019</a></li>
        <li> <a href="https://cefrl.webflow.io/" class="newfont">2<sup>nd</sup> CEFRL Workshop in conjunction with ECCV 2018</a></li>
        <li> <a href="http://www.ee.oulu.fi/~lili/ICCVW2017.html" class="newfont">1<sup>st</sup> CEFRL Workshop in conjunction with ICCV 2017</a></li>
		<li><a href="https://icml.cc/Conferences/2019/Schedule?showEvent=3520" class="newfont">ICML 2019: Joint Workshop on On-Device Machine Learning and Compact Deep Neural Network Representations (ODML-CDNNR)</a></li>
        <li> <a href="https://nips.cc/Conferences/2018/Schedule?showEvent=10941" class="newfont">NIPS 2018: workshop on Compact Deep Neural Networks with industrial applications</a></li>
        <li><a href="https://sites.google.com/view/ecv2019/" class="newfont">2<sup>nd</sup> Efficient Deep Learning for Computer Vision
          in conjunction with CVPR 2019</a></li>
        <li><a href="https://sites.google.com/view/ecv2018/" class="newfont">1<sup>st</sup> Efficient Deep Learning for Computer Vision
          in conjunction with CVPR 2018</a></li>
		  </ul>
        
      </div>
    </div>
  </div>
</div>

<div class="section-6 w-hidden-small w-hidden-tiny">
  <div class="w-container">
    <div>
      <h2 class="heading-7" align="center">Main Contacts</h2>
      <div class="newfontsize">If you have question, please contact : </br></br>
		<ul>
		<li><strong><em>Dr. Li Liu</em></strong> : li.liu@oulu.fi</li></br>
		<li><strong><em >Dr. Peter Vajda</em></strong> : vajdap@fb.com </li> </br>
		<li><strong><em>Dr. Werner Bailer</em></strong> : werner.bailer@joanneum.at</li>
		</ul>
		  </br>
    </div>
  </div>
</div>
<div class="section-6 w-hidden-main w-hidden-medium">
  <div class="w-container">
    <div class="div-block">
      <h2 class="heading-7" align="center">Main Contacts</h2>
      <div class="newfontsize">If you have question, please contact : </br></br>
		<ul>
		<li><strong><em>Dr. Li Liu</em></strong> : li.liu@oulu.fi</li></br>
		<li><strong><em >Dr. Peter Vajda</em></strong> : vajdap@fb.com </li> </br>
		<li><strong><em>Dr. Werner Bailer</em></strong> : werner.bailer@joanneum.at</li>
		</ul>
		  </br>
    </div>
    </div>
  </div>
</div>


<div class="section footer">
  <div class="w-container">
    <div class="w-row">
      <div class="w-col w-col-3">
        <div class="logo-text footer">EDLCV 2020</div>
        <div class="social-icon-group"> 
			<a href="#" class="social-icon w-inline-block"><img src="https://uploads-ssl.webflow.com/5abe7e9055d981785ad0393e/5abe7e9055d981965fd03963_facebook-icon.svg" alt=""/></a> 
			<a href="#" class="social-icon w-inline-block"><img src="https://uploads-ssl.webflow.com/5abe7e9055d981785ad0393e/5abe7e9055d981167bd03965_twitter-icon.svg" alt=""/></a> 
			<a href="#" class="social-icon w-inline-block"><img src="https://uploads-ssl.webflow.com/5abe7e9055d981785ad0393e/5abe7e9055d98147a9d03966_linkdin-icon-white.svg" alt=""/></a> 
			<a href="#" class="social-icon w-inline-block"><img src="https://uploads-ssl.webflow.com/5abe7e9055d981785ad0393e/5abe7e9055d981a51dd0395b_email-icon-white.svg" alt=""/></a> 
		  </div>
      </div>
      <div class="w-col w-col-9"></div>
    </div>
  </div>
</div>
<script src="https://code.jquery.com/jquery-3.3.1.min.js" type="text/javascript" integrity="sha256-FgpCb/KJQlLNfOu91ta32o/NMZxltwRo8QtmkMRdAu8=" crossorigin="anonymous"></script><script src="https://uploads-ssl.webflow.com/5abe7e9055d981785ad0393e/js/webflow.2e96abf72.js" type="text/javascript"></script><!--[if lte IE 9]><script src="//cdnjs.cloudflare.com/ajax/libs/placeholders/3.0.2/placeholders.min.js"></script><![endif]-->
</body>
</html>
